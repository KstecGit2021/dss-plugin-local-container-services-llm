{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패키지 임포트\n",
    "import dataiku  # Dataiku 라이브러리 임포트\n",
    "from dataiku import pandasutils as pdu  # Pandas 유틸리티를 Dataiku에서 임포트\n",
    "from dataiku.snowpark import DkuSnowpark  # Dataiku Snowpark 클래스 임포트\n",
    "\n",
    "import pandas as pd  # Pandas 데이터 처리 라이브러리 임포트\n",
    "import torch  # PyTorch 딥러닝 라이브러리 임포트\n",
    "import importlib  # 동적 모듈 임포트를 위한 라이브러리\n",
    "import json  # JSON 데이터 처리를 위한 라이브러리\n",
    "import subprocess  # 하위 프로세스를 관리하기 위한 라이브러리\n",
    "import requests  # HTTP 요청을 위한 라이브러리\n",
    "import logging  # 로깅을 위한 라이브러리\n",
    "\n",
    "import transformers  # Hugging Face의 Transformers 라이브러리 임포트\n",
    "from transformers import pipeline  # 파이프라인 API 임포트\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # 자동 토크나이저 및 대화형 모델 클래스 임포트\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # Hugging Face 임베딩을 위한 LangChain 라이브러리 임포트\n",
    "\n",
    "from snowflake.ml.registry import model_registry  # Snowflake ML 모델 레지스트리 임포트\n",
    "from snowflake.ml.model import deploy_platforms  # Snowflake ML 모델 배포 플랫폼 임포트\n",
    "from snowflake.ml.model.models import huggingface_pipeline  # Hugging Face 파이프라인 모델 임포트\n",
    "from snowflake.ml.model.models import llm  # LLM(대형 언어 모델) 관련 임포트\n",
    "from snowflake.ml.model import custom_model  # 사용자 정의 모델 관련 임포트\n",
    "from snowflake.ml.model import model_signature  # 모델 서명 관련 임포트\n",
    "\n",
    "from snowflake.snowpark.functions import lit  # Snowpark 함수에서 리터럴(literal) 함수 임포트\n",
    "import snowflake.connector  # Snowflake 데이터베이스 연결을 위한 라이브러리 임포트\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 이름, GPU 수, 최대 토큰 수 등을 제어하는 파라미터 설정\n",
    "\n",
    "# Snowflake 모델 레지스트리에 LLM을 배포할 수 있는 데이터 아이쿠 관리 UI에서의 Snowflake 연결 이름\n",
    "snowflake_connection_name = \"spcs-access-only\"  # Snowflake 연결 이름\n",
    "\n",
    "# 배포할 주요 LLM의 이름\n",
    "main_llm_model_name = \"ZEPHYRBeta\"  # 또는 \"LLAMA2\" 또는 \"FALCON\" 또는 \"ZEPHYRBeta\" 또는 \"Phi2\"\n",
    "\n",
    "# 주요 LLM을 배포할 SPCS 컴퓨트 풀 - Snowflake 측에서 설정되어 있어야 함\n",
    "main_llm_compute_pool = \"DATAIKU_GPU_NV_S_MODEL_COMPUTE_POOL\"  # 메인 LLM을 배포할 SPCS 컴퓨트 풀 이름\n",
    "\n",
    "embedding_llm_model_name = \"MiniLM-L6-v2\"  # 배포할 임베딩 LLM 모델 이름\n",
    "\n",
    "# 임베딩 LLM을 배포할 SPCS 컴퓨트 풀 - Snowflake 측에서 설정되어 있어야 함\n",
    "embedding_llm_compute_pool = \"DATAIKU_CPU_X64_XS_EMBED_COMPUTE_POOL\" \n",
    "\n",
    "# LLM을 등록할 Snowflake 모델 레지스트리 DB 및 스키마\n",
    "database_name = \"DATAIKU_SPCS\"  # Snowflake 모델 레지스트리 데이터베이스 이름\n",
    "schema_name = \"MODEL_REGISTRY\"  # Snowflake 모델 레지스트리 스키마 이름\n",
    "\n",
    "external_access_integrations = [\"SNOWFLAKE_EGRESS_ACCESS_INTEGRATION\"]  # Snowflake 외부 접근 통합 이름\n",
    "\n",
    "# LLM과 관련된 추가 파라미터 설정\n",
    "\n",
    "main_llm_num_gpus = 1  # 메인 LLM에 사용 가능한 GPU 수\n",
    "\n",
    "main_llm_max_instances = 1  # 메인 LLM의 최대 인스턴스 수\n",
    "\n",
    "embedding_llm_num_gpus = 0  # 임베딩 LLM에 사용 가능한 GPU 수\n",
    "\n",
    "embedding_llm_max_instances = 1  # 임베딩 LLM의 최대 인스턴스 수\n",
    "\n",
    "max_new_tokens = 200  # 메인 LLM에서 생성할 최대 새로운 토큰 수\n",
    "\n",
    "# Hugging Face 토큰 가져오기\n",
    "# hugging_face_token = \"<YOUR_HF_TOKEN>\"\n",
    "hugging_face_token = dataiku.get_custom_variables()[\"hugging_face_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowpark의 DSS 래퍼 생성\n",
    "dku_snowpark = DkuSnowpark()\n",
    "snowpark_session = dku_snowpark.get_session(snowflake_connection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_main_llm_to_spcs(modelname, snowparksession, computepool, maxnewtokens, numgpus, maxinstances, databasename, schemaname, hftoken, externalaccessintegrations):\n",
    "    \"\"\"\n",
    "    Snowpark ML을 사용하여 SPCS에 채팅 완료 LLM 엔드포인트를 배포합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 모델 이름과 실제 HuggingFace 이름, 토크나이저(필요한 경우), pip/conda 의존성을 매핑\n",
    "    model_name_hf_mapping = {\n",
    "        \"FALCON\": {\n",
    "            \"hf_name\": \"tiiuae/falcon-7b-instruct\",  # HuggingFace 모델 이름\n",
    "            \"tokenizer\": AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b-instruct\"),  # 토크나이저\n",
    "            \"token\": None,  # 인증 토큰 (필요한 경우)\n",
    "            \"deployment_name\": \"falcon_7b_predict\",  # 배포 이름\n",
    "            \"pip_requirements\": [\"einops\",\"snowflake-snowpark-python==1.9.0\"],  # pip 요구 사항\n",
    "            \"conda_dependencies\": None  # conda 의존성\n",
    "        },\n",
    "        \"LLAMA2\": {\n",
    "            \"hf_name\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "            \"tokenizer\": None,\n",
    "            \"token\": hftoken,\n",
    "            \"deployment_name\": \"llama2_7b_predict\",\n",
    "            \"pip_requirements\": None,\n",
    "            \"conda_dependencies\": [\"snowflake-snowpark-python==1.9.0\"]\n",
    "        },\n",
    "        \"ZEPHYRBeta\": {\n",
    "            \"hf_name\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "            \"tokenizer\": None,\n",
    "            \"token\": None,\n",
    "            \"deployment_name\": \"zephyr_beta_7b_predict\",\n",
    "            \"pip_requirements\": None,\n",
    "            \"conda_dependencies\": [\"snowflake-snowpark-python==1.9.0\", \"transformers==4.34.1\"]\n",
    "        },\n",
    "        \"Phi2\": {\n",
    "            \"hf_name\": \"microsoft/phi-2\",\n",
    "            \"tokenizer\": None,\n",
    "            \"token\": None,\n",
    "            \"deployment_name\": \"phi2_predict\",\n",
    "            \"pip_requirements\": None,\n",
    "            \"conda_dependencies\": [\"snowflake-snowpark-python==1.9.0\", \"transformers==4.37.1\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 모델 이름에 대한 Hugging Face 이름, 토크나이저, 토큰을 가져오기\n",
    "    hf_model_name = model_name_hf_mapping[modelname][\"hf_name\"]\n",
    "    tokenizer = model_name_hf_mapping[modelname][\"tokenizer\"]\n",
    "    token = model_name_hf_mapping[modelname][\"token\"]\n",
    "\n",
    "    # HuggingFace 파이프라인 모델을 가져옴\n",
    "    hf_model = huggingface_pipeline.HuggingFacePipelineModel(\n",
    "        task=\"text-generation\",  # 작업 유형\n",
    "        model=hf_model_name,  # 모델 이름\n",
    "        tokenizer=tokenizer,  # 토크나이저\n",
    "        token=token,  # 인증 토큰\n",
    "        trust_remote_code=True,  # 원격 코드 신뢰 설정\n",
    "        return_full_text=False,  # 전체 텍스트 반환 여부\n",
    "        max_new_tokens=maxnewtokens,  # 생성할 최대 새로운 토큰 수\n",
    "        device_map=\"auto\",  # 장치 맵 자동 설정\n",
    "        model_kwargs={\"load_in_8bit\": True}  # 모델 로딩 옵션\n",
    "    )\n",
    "\n",
    "    # Snowflake 모델 레지스트리 가져오기 (존재하지 않으면 생성)\n",
    "    registry = model_registry.ModelRegistry(\n",
    "        session=snowparksession,\n",
    "        database_name=databasename,\n",
    "        schema_name=schemaname,\n",
    "        create_if_not_exists=True # 존재하지 않으면 생성\n",
    "    )\n",
    "\n",
    "    # 모델 버전 1\n",
    "    model_version = \"1\"\n",
    "\n",
    "    # 레지스트리에 이전 모델 버전 삭제 시도\n",
    "    try:\n",
    "        registry.delete_model(model_name=modelname, model_version=model_version)\n",
    "    except:\n",
    "        pass # 삭제 중 오류가 발생하면 무시\n",
    "\n",
    "    # Snowflake 모델 레지스트리에 모델 기록하기\n",
    "    hf_model_registry = registry.log_model(\n",
    "        model_name=modelname,\n",
    "        model_version=model_version,\n",
    "        model=hf_model,\n",
    "        pip_requirements=model_name_hf_mapping[modelname][\"pip_requirements\"],\n",
    "        conda_dependencies=model_name_hf_mapping[modelname][\"conda_dependencies\"]\n",
    "    )\n",
    "\n",
    "    # 레지스트리에서 SPCS 컴퓨트 풀에 모델 배포\n",
    "    deployed_model = hf_model_registry.deploy(\n",
    "        deployment_name=model_name_hf_mapping[modelname][\"deployment_name\"],\n",
    "        platform=deploy_platforms.TargetPlatform.SNOWPARK_CONTAINER_SERVICES,\n",
    "        options={\n",
    "            \"compute_pool\": computepool,  # 컴퓨트 풀 이름\n",
    "            \"num_gpus\": numgpus,  # 사용할 GPU 수\n",
    "            \"max_instances\": maxinstances,  # 최대 인스턴스 수\n",
    "            \"enable_ingress\": True,  # 인그레스 활성화 여부\n",
    "            \"external_access_integrations\": externalaccessintegrations  # 외부 접근 통합\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"메인 LLM을 SPCS에 배포했습니다.\")\n",
    "    return deployed_model # 배포된 모델 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_embedding_llm_to_spcs(modelname, snowparksession, computepool, numgpus, maxinstances, databasename, schemaname, hftoken, externalaccessintegrations):\n",
    "    \"\"\"\n",
    "    Snowpark ML을 사용하여 SPCS에 임베딩 LLM 엔드포인트를 배포합니다.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 모델 이름 (현재는 하나만 작성됨)과 Hugging Face 이름 및 pip/conda 의존성을 매핑\n",
    "    model_name_hf_mapping = {\n",
    "        \"MiniLM-L6-v2\": {\n",
    "            \"hf_name\": \"sentence-transformers/all-MiniLM-L6-v2\",  # HuggingFace 모델 이름\n",
    "            \"deployment_name\": \"MiniLM_L6_v2_embed\",  # 배포 이름\n",
    "            \"pip_requirements\": None,  # pip 요구 사항\n",
    "            \"conda_dependencies\": [\"snowflake-snowpark-python==1.9.0\", \"transformers==4.34.1\", \"sentence-transformers==2.2.2\", \"langchain\"]  # conda 의존성\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 모델 이름이 \"MiniLM-L6-v2\"인 경우 HuggingFace 모델 이름 설정\n",
    "    if modelname == \"MiniLM-L6-v2\":\n",
    "        hf_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "    # LLM을 감싸고 SPCS 표준에 맞게 조정하는 사용자 정의 임베딩 LLM 클래스 생성\n",
    "    class EmbeddingLLMCustom(custom_model.CustomModel):\n",
    "        def __init__(self, context: custom_model.ModelContext) -> None:\n",
    "            super().__init__(context)  # 부모 클래스 초기화\n",
    "\n",
    "            # HuggingFace 임베딩 초기화\n",
    "            self.embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=model_name_hf_mapping[modelname][\"hf_name\"],  # HuggingFace 모델 이름\n",
    "            )\n",
    "\n",
    "        @custom_model.inference_api  # 추론 API 데코레이터\n",
    "        def predict(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "            # 입력 텍스트를 임베딩하는 내부 함수 정의\n",
    "            def _embed(input_text: str) -> str:\n",
    "                query_result = self.embeddings.embed_query(input_text)  # 쿼리 결과 생성\n",
    "                return str(query_result)  # 결과를 문자열로 반환\n",
    "\n",
    "            # 입력 데이터 프레임의 각 입력에 대해 _embed 함수 호출\n",
    "            res_df = pd.DataFrame({\"outputs\": pd.Series.apply(X[\"inputs\"], _embed)})\n",
    "            return res_df  # 결과 데이터 프레임 반환\n",
    "\n",
    "\n",
    "    # 사용자 정의 LLM 클래스 가져오기\n",
    "    hf_model = EmbeddingLLMCustom(custom_model.ModelContext())\n",
    "\n",
    "    # Snowflake 모델 레지스트리 가져오기 (존재하지 않으면 생성)\n",
    "    registry = model_registry.ModelRegistry(\n",
    "        session=snowparksession,\n",
    "        database_name=databasename,\n",
    "        schema_name=schemaname,\n",
    "        create_if_not_exists=True  # 존재하지 않으면 생성\n",
    "    )\n",
    "\n",
    "    # 모델 버전 1\n",
    "    model_version = \"1\"\n",
    "\n",
    "    # 레지스트리에 이미 모델이 존재하면 이전 버전을 삭제\n",
    "    try:\n",
    "        registry.delete_model(model_name=modelname, model_version=model_version)\n",
    "    except:\n",
    "        pass  # 삭제 중 오류가 발생하면 무시\n",
    "\n",
    "    # Snowflake 모델 레지스트리에 모델 기록하기\n",
    "    hf_model_registry = registry.log_model(\n",
    "        model_name=modelname,\n",
    "        model_version=model_version,\n",
    "        model=hf_model,\n",
    "        conda_dependencies=model_name_hf_mapping[modelname][\"conda_dependencies\"],\n",
    "        signatures={\n",
    "            \"predict\": model_signature.ModelSignature(\n",
    "                inputs=[model_signature.FeatureSpec(name=\"inputs\", dtype=model_signature.DataType.STRING)],\n",
    "                outputs=[model_signature.FeatureSpec(name=\"outputs\", dtype=model_signature.DataType.STRING)],\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 레지스트리에서 선택한 SPCS 컴퓨트 풀에 모델을 배포\n",
    "    deployed_model = hf_model_registry.deploy(\n",
    "        deployment_name=model_name_hf_mapping[modelname][\"deployment_name\"],  # 배포 이름\n",
    "        platform=deploy_platforms.TargetPlatform.SNOWPARK_CONTAINER_SERVICES,  # 배포 플랫폼\n",
    "        options={\n",
    "            \"compute_pool\": computepool,  # 컴퓨트 풀 이름\n",
    "            \"max_instances\": maxinstances,  # 최대 인스턴스 수\n",
    "            \"num_gpus\": numgpus,  # 사용할 GPU 수\n",
    "            \"enable_ingress\": True,  # 인그레스 활성화 여부\n",
    "            \"external_access_integrations\": externalaccessintegrations  # 외부 접근 통합\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"SPCS에 임베딩 LLM을 배포했습니다.\")  # 배포 완료 메시지 출력\n",
    "\n",
    "    return deployed_model  # 배포된 모델 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메인 LLM을 SPCS에 배포\n",
    "main_llm_on_spcs = deploy_main_llm_to_spcs(\n",
    "    main_llm_model_name,  # 배포할 메인 LLM 모델 이름\n",
    "    snowpark_session,  # Snowpark 세션\n",
    "    main_llm_compute_pool,  # 메인 LLM을 배포할 컴퓨트 풀\n",
    "    max_new_tokens,  # 메인 LLM이 생성할 최대 새로운 토큰 수\n",
    "    main_llm_num_gpus,  # 메인 LLM에 사용할 GPU 수\n",
    "    main_llm_max_instances,  # 메인 LLM의 최대 인스턴스 수\n",
    "    database_name,  # 모델 레지스트리 데이터베이스 이름\n",
    "    schema_name,  # 모델 레지스트리 스키마 이름\n",
    "    hugging_face_token,  # Hugging Face API 토큰\n",
    "    external_access_integrations  # 외부 접근 통합 이름\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 LLM을 SPCS에 배포\n",
    "embedding_llm_on_spcs = deploy_embedding_llm_to_spcs(\n",
    "    embedding_llm_model_name,  # 배포할 임베딩 LLM 모델 이름\n",
    "    snowpark_session,  # Snowpark 세션\n",
    "    embedding_llm_compute_pool,  # 임베딩 LLM을 배포할 컴퓨트 풀\n",
    "    embedding_llm_num_gpus,  # 임베딩 LLM에 사용할 GPU 수\n",
    "    embedding_llm_max_instances,  # 임베딩 LLM의 최대 인스턴스 수\n",
    "    database_name,  # 모델 레지스트리 데이터베이스 이름\n",
    "    schema_name,  # 모델 레지스트리 스키마 이름\n",
    "    hugging_face_token,  # Hugging Face API 토큰\n",
    "    external_access_integrations  # 외부 접근 통합 이름\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메인 및 임베딩 LLM의 SPCS에서의 공개 URL 가져오기\n",
    "def get_llm_endpoint_urls(snowparksession, main_llm, embedding_llm):\n",
    "    try:\n",
    "        # 메인 LLM 서비스 이름 가져오기\n",
    "        main_llm_service_name = main_llm['details']['service_info']['name']\n",
    "        # 서비스의 엔드포인트를 보여주는 쿼리 생성\n",
    "        show_services_query = \"SHOW ENDPOINTS IN SERVICE \" + main_llm_service_name\n",
    "        # 쿼리 실행 결과 가져오기\n",
    "        show_services_query_result = snowparksession.sql(show_services_query)\n",
    "        service = show_services_query_result.collect()[0]  # 첫 번째 서비스 정보 추출\n",
    "        spcs_service_url = service['ingress_url']  # 서비스의 인그레스 URL 가져오기\n",
    "        spcs_service_url_full = \"https://\" + spcs_service_url  # 전체 URL 생성\n",
    "        print(\"채팅 완료 LLM URL: \" + spcs_service_url_full)  # 메인 LLM URL 출력\n",
    "    except:\n",
    "        # URL 생성 중 오류 발생 시 메시지 출력\n",
    "        print(\"채팅 완료 URL이 없습니다. 공개 URL 생성에 1분 정도 걸릴 수 있습니다... 다시 시도해 주세요.\")\n",
    "\n",
    "    try:\n",
    "        # 임베딩 LLM 서비스 이름 가져오기\n",
    "        embedding_llm_service_name = embedding_llm['details']['service_info']['name']\n",
    "        # 서비스의 엔드포인트를 보여주는 쿼리 생성\n",
    "        show_services_query = \"SHOW ENDPOINTS IN SERVICE \" + embedding_llm_service_name\n",
    "        # 쿼리 실행 결과 가져오기\n",
    "        show_services_query_result = snowparksession.sql(show_services_query)\n",
    "        service = show_services_query_result.collect()[0]  # 첫 번째 서비스 정보 추출\n",
    "        spcs_service_url = service['ingress_url']  # 서비스의 인그레스 URL 가져오기\n",
    "        spcs_service_url_full = \"https://\" + spcs_service_url  # 전체 URL 생성\n",
    "        print(\"텍스트 임베딩 LLM URL: \" + spcs_service_url_full)  # 임베딩 LLM URL 출력\n",
    "    except:\n",
    "        # URL 생성 중 오류 발생 시 메시지 출력\n",
    "        print(\"텍스트 임베딩 URL이 없습니다. 공개 URL 생성에 1분 정도 걸릴 수 있습니다... 다시 시도해 주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM 엔드포인트 URL 가져오기\n",
    "get_llm_endpoint_urls(snowpark_session, main_llm_on_spcs, embedding_llm_on_spcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "createdOn": 1695053827986,
  "creator": "pat",
  "customFields": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python (env py_38_snowpark_llms)",
   "language": "python",
   "name": "py-dku-venv-py_38_snowpark_llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "modifiedBy": "pat",
  "tags": []
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
